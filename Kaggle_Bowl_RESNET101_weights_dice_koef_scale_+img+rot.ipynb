{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'add_newdocs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6d164e95c7cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpackages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd_newdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     __all__ = ['add_newdocs',\n\u001b[0;32m    144\u001b[0m                \u001b[1;34m'ModuleDeprecationWarning'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'add_newdocs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set some parameters\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "TRAIN_PATH = 'G:/Kaggle_data/Kaggle_bowl/stage1_train/'\n",
    "TEST_PATH = 'G:/Kaggle_data/Kaggle_bowl/stage1_test/'\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "time = time.strftime(\"%Y-%m-%d\", time.gmtime())\n",
    "\n",
    "smooth = 1\n",
    "\n",
    "# Get train and test IDs\n",
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "test_ids = next(os.walk(TEST_PATH))[1]\n",
    "\n",
    "# Get and resize train images and masks\n",
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[n] = img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "        mask_ = imread(path + '/masks/' + mask_file)\n",
    "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                      preserve_range=True), axis=-1)\n",
    "        mask = np.maximum(mask, mask_)\n",
    "    Y_train[n] = mask\n",
    "    \n",
    "#def scale_img_canals(an_img):\n",
    "for i in range(IMG_CHANNELS):\n",
    "    canal = img[:,:,i]\n",
    "    canal = canal - canal.min()\n",
    "    canalmax = canal.max()\n",
    "    if canalmax > 0:\n",
    "        factor = 255/canalmax\n",
    "        canal = (canal * factor).astype(int)\n",
    "    img[:,:,i] = canal\n",
    "\n",
    "# Get and resize test images\n",
    "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "sizes_test = []\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = TEST_PATH + id_\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "    sizes_test.append([img.shape[0], img.shape[1]])\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_test[n] = img\n",
    "    \n",
    "for i in range(IMG_CHANNELS):\n",
    "    canal = img[:,:,i]\n",
    "    canal = canal - canal.min()\n",
    "    canalmax = canal.max()\n",
    "    if canalmax > 0:\n",
    "        factor = 255/canalmax\n",
    "        canal = (canal * factor).astype(int)\n",
    "    img[:,:,i] = canal\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    rotated_images_pos90 = []\n",
    "    rotated_images_neg90 = []\n",
    "    \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "#         IMG_HEIGHT = imgs.shape[0]\n",
    "#         IMG_WIDTH = imgs.shape[1]\n",
    "    \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        arp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        arp90_r = cv2.warpAffine(a,arp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        arn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),270,1)\n",
    "        arn90_r = cv2.warpAffine(a,arn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        brp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        brp90_r = cv2.warpAffine(a,brp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        brn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),270,1)\n",
    "        brn90_r = cv2.warpAffine(a,brn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        crp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        crp90_r = cv2.warpAffine(a,crp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        crn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),270,1)\n",
    "        crn90_r = cv2.warpAffine(a,crn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "        rotated_images_pos90.append(np.dstack((arp90_r, brp90_r, crp90_r)))\n",
    "        rotated_images_neg90.append(np.dstack((arn90_r, brn90_r, crn90_r)))\n",
    "    \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    rp = np.array(rotated_images_pos90)\n",
    "    rn = np.array(rotated_images_neg90)\n",
    "    \n",
    "    print(v)\n",
    "#     print('h shape \\n' + h)\n",
    "    print(rp)\n",
    "#     print('rn shape \\n' + rn)\n",
    "    more_images = np.concatenate((imgs,v,h,rp,rn))\n",
    "    \n",
    "    return more_images\n",
    "\n",
    "def duplicate_labels(labels):\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    rotated_images_pos90 = []\n",
    "    rotated_images_neg90 = []\n",
    "    for i in range(0,labels.shape[0]):\n",
    "        \n",
    "#         IMG_HEIGHT = labels.shape[0]\n",
    "#         IMG_WIDTH = labels.shape[1]\n",
    "        \n",
    "        a=labels[i,:,:,0]\n",
    "\n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        \n",
    "        arp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        arp90_r = cv2.warpAffine(a,arp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        arn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),-90,1)\n",
    "        arn90_r = cv2.warpAffine(a,arn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        vert_flip_imgs.append(av.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        hori_flip_imgs.append(ah.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        rotated_images_pos90.append(arp90_r.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        rotated_images_neg90.append(arn90_r.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    rp = np.array(rotated_images_pos90)\n",
    "    rn = np.array(rotated_images_neg90)\n",
    "    duplicate_labels = np.concatenate((labels,v,h,rp,rn))\n",
    "    return duplicate_labels\n",
    "\n",
    "X_validation = X_train[-50:,...]\n",
    "Y_validation = Y_train[-50:]\n",
    "X_train = X_train[0:-50,...]\n",
    "Y_train = Y_train[0:-50]\n",
    "print(\"Validation Set of Size \"+str(Y_validation.shape[0])+\" Separated\")\n",
    "Y_train.dtype = np.uint8\n",
    "X_train = get_more_images(X_train)\n",
    "Y_train = duplicate_labels(Y_train)\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_validation.shape)\n",
    "print (Y_validation.shape)\n",
    "print(\"Data Rotation and Flipping Complete\")\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_validation.shape)\n",
    "print (Y_validation.shape)\n",
    "X_train = np.concatenate((X_train,X_validation),axis=0)\n",
    "Y_train = np.concatenate((Y_train,Y_validation),axis=0)\n",
    "np.savez_compressed(file='train.npz',X=X_train,Y=Y_train)\n",
    "np.savez_compressed(file='test.npz',X=X_test)\n",
    "\n",
    "# npz = np.load('G:/Kaggle_data/Kaggle_bowl/train.npz') \n",
    "# X_train2 = npz['X'] \n",
    "# Y_train2 = npz['Y']\n",
    "# X_test2 = np.load('G:/Kaggle_data/Kaggle_bowl/test.npz')['X']\n",
    "\n",
    "# Check if training data looks all right\n",
    "# ix = random.randint(0, len(train_ids))\n",
    "# imshow(X_train[ix])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[ix]))\n",
    "# plt.show()\n",
    "\n",
    "# Define IoU metric\n",
    "\"\"\"def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\"\"\"\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "# Build U-Net model\n",
    "# inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "# s = Lambda(lambda x: x / 255) (inputs)\n",
    "\n",
    "# c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "# c1 = Dropout(0.1) (c1)\n",
    "# c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "# p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "# c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "# c2 = Dropout(0.1) (c2)\n",
    "# c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "# p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "# c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "# c3 = Dropout(0.2) (c3)\n",
    "# c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "# p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "# c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "# c4 = Dropout(0.2) (c4)\n",
    "# c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "# p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "# c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "# c5 = Dropout(0.3) (c5)\n",
    "# c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "# u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "# u6 = concatenate([u6, c4])\n",
    "# c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "# c6 = Dropout(0.2) (c6)\n",
    "# c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "# u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "# u7 = concatenate([u7, c3])\n",
    "# c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "# c7 = Dropout(0.2) (c7)\n",
    "# c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "# u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "# u8 = concatenate([u8, c2])\n",
    "# c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "# c8 = Dropout(0.1) (c8)\n",
    "# c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "# u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "# u9 = concatenate([u9, c1], axis=3)\n",
    "# c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "# c9 = Dropout(0.1) (c9)\n",
    "# c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "# outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "# model = Model(inputs=[inputs], outputs=[outputs])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])\n",
    "# model.summary()\n",
    "\n",
    "# Fit model\n",
    "# earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "# checkpointer = ModelCheckpoint('model-dsbowl2018-1_+img_rotated.h5', verbose=1, save_best_only=True)\n",
    "# results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=16, epochs=50, \n",
    "#                     callbacks=[earlystopper, checkpointer])\n",
    "# results2 = model.fit(X_train2, Y_train2, validation_split=0.1, batch_size=16, epochs=50, \n",
    "#                     callbacks=[earlystopper, checkpointer])\n",
    "\n",
    "# Predict on train, val and test\n",
    "#model = load_model('model-dsbowl2018-1_+img_rotated.h5', custom_objects={'dice_coef': dice_coef})\n",
    "model = load_model('resnet101_weights_tf.h5', custom_objects={'dice_coef': dice_coef})\n",
    "preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
    "preds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\n",
    "preds_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "# preds_train2 = model.predict(X_train2[:int(X_train2.shape[0]*0.9)], verbose=1)\n",
    "# preds_val2 = model.predict(X_train2[int(X_train2.shape[0]*0.9):], verbose=1)\n",
    "# preds_test2 = model.predict(X_test2, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "\n",
    "# preds_train_t2 = (preds_train2 > 0.5).astype(np.uint8)\n",
    "# preds_val_t2 = (preds_val2 > 0.5).astype(np.uint8)\n",
    "# preds_test_t2 = (preds_test2 > 0.5).astype(np.uint8)\n",
    "\n",
    "# Create list of upsampled test masks\n",
    "preds_test_upsampled = []\n",
    "for i in range(len(preds_test)):\n",
    "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
    "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
    "                                       mode='constant', preserve_range=True))\n",
    "# for i in range(len(preds_test2)):\n",
    "#     preds_test_upsampled.append(resize(np.squeeze(preds_test2[i]), \n",
    "#                                        (sizes_test[i][0], sizes_test[i][1]), \n",
    "#                                        mode='constant', preserve_range=True))\n",
    "    \n",
    "# Perform a sanity check on some random training samples\n",
    "# ix = random.randint(0, len(preds_train_t))\n",
    "# imshow(X_train[ix])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[ix]))\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(preds_train_t[ix]))\n",
    "# plt.show()\n",
    "\n",
    "# Perform a sanity check on some random validation samples\n",
    "# ix = random.randint(0, len(preds_val_t))\n",
    "# imshow(X_train[int(X_train.shape[0]*0.9):][ix])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[int(Y_train.shape[0]*0.9):][ix]))\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(preds_val_t[ix]))\n",
    "# plt.show()\n",
    "\n",
    "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def prob_to_rles(x, cutoff=0.5):\n",
    "    lab_img = label(x > cutoff)\n",
    "    for i in range(1, lab_img.max() + 1):\n",
    "        yield rle_encoding(lab_img == i)\n",
    "        \n",
    "new_test_ids = []\n",
    "rles = []\n",
    "for n, id_ in enumerate(test_ids):\n",
    "    rle = list(prob_to_rles(preds_test_upsampled[n]))\n",
    "    rles.extend(rle)\n",
    "    new_test_ids.extend([id_] * len(rle))\n",
    "    \n",
    "# Create submission DataFrame\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ImageId'] = new_test_ids\n",
    "sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
    "sub.to_csv('sub-dsbowl2018-1_%s_256_scale+flip_rot_RESNET101.csv' % time, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 54s - ETA: 49 - ETA: 43 - ETA: 37 - ETA: 32 - ETA: 26 - ETA: 20 - ETA: 15 - ETA: 9 - ETA:  - 302s 176ms/step\n",
      "191/191 [==============================] - ETA: 26 - ETA: 21 - ETA: 16 - ETA: 10 - ETA: 5 - 32s 169ms/step\n",
      "65/65 [==============================] - ETA:  - ETA:  - 11s 169ms/step\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
