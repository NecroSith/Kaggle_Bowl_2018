{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from textwrap import wrap\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "# def read_image(filepath, color_mode=cv2.IMREAD_COLOR, target_size=None,space='bgr'):\n",
    "#     \"\"\"Read an image from a file and resize it.\"\"\"\n",
    "#     img = cv2.imread(filepath, color_mode)\n",
    "#     if target_size: \n",
    "#         img = cv2.resize(img, target_size, interpolation = cv2.INTER_AREA)\n",
    "#     if space == 'hsv':\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def load_raw_data(image_size=(256, 256), space = 'bgr',load_mask=True):\n",
    "#     \"\"\"Load raw data.\"\"\"\n",
    "#     # Python lists to store the training images/masks and test images.\n",
    "#     x_train, y_train, x_test = [],[],[]\n",
    "\n",
    "#     # Read and resize train images/masks. \n",
    "#     print('Loading and resizing train images and masks ...')\n",
    "#     print (train_df['image_path'])\n",
    "#     sys.stdout.flush()\n",
    "#     for i, filename in tqdm.tqdm(enumerate(train_df['image_path']), total=len(train_df)):\n",
    "#         img = read_image(train_df['image_path'].loc[i], target_size=image_size,space = space)\n",
    "#         if load_mask:\n",
    "#             mask = read_image(train_df['mask_path'].loc[i],\n",
    "#                               color_mode=cv2.IMREAD_GRAYSCALE,\n",
    "#                               target_size=image_size)\n",
    "#             #mask = read_mask(train_df['mask_dir'].loc[i], target_size=image_size)\n",
    "#             y_train.append(mask)\n",
    "#         x_train.append(img)\n",
    "        \n",
    "#     # Read and resize test images. \n",
    "#     print('Loading and resizing test images ...')\n",
    "#     sys.stdout.flush()\n",
    "#     for i, filename in tqdm.tqdm(enumerate(test_df['image_path']), total=len(test_df)):\n",
    "#         img = read_image(test_df['image_path'].loc[i], target_size=image_size,space=space)\n",
    "#         x_test.append(img)\n",
    "\n",
    "#     # Transform lists into 4-dim numpy arrays.\n",
    "#     x_train = np.array(x_train)\n",
    "#     #if load_mask:\n",
    "#     y_train = np.array(y_train)\n",
    "#     #y_train = np.expand_dims(np.array(y_train), axis=4)\n",
    "#     x_test = np.array(x_test)\n",
    "#     print('Data loaded')\n",
    "#     if load_mask:\n",
    "#         return x_train, y_train, x_test\n",
    "#     else:\n",
    "#         return x_train, x_test\n",
    "\n",
    "\n",
    "# Set some parameters\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "TRAIN_PATH = 'G:/Kaggle_data/Kaggle_bowl/stage1_train/'\n",
    "TEST_PATH = 'G:/Kaggle_data/Kaggle_bowl/stage2_test_final/'\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 666\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "time = time.strftime(\"%Y-%m-%d\", time.gmtime())\n",
    "\n",
    "smooth = 1\n",
    "\n",
    "def get_images():\n",
    "    # Get train and test IDs\n",
    "    train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "    test_ids = next(os.walk(TEST_PATH))[1]\n",
    "\n",
    "    # Get and resize train images and masks\n",
    "    X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    print('Getting and resizing train images and masks ... ')\n",
    "    sys.stdout.flush()\n",
    "    for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "        path = TRAIN_PATH + id_\n",
    "        img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        X_train[n] = img\n",
    "        mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "        for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "            mask_ = imread(path + '/masks/' + mask_file)\n",
    "            mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                          preserve_range=True), axis=-1)\n",
    "            mask = np.maximum(mask, mask_)\n",
    "        Y_train[n] = mask\n",
    "\n",
    "    #def scale_img_canals(an_img):\n",
    "    for i in range(IMG_CHANNELS):\n",
    "        canal = img[:,:,i]\n",
    "        canal = canal - canal.min()\n",
    "        canalmax = canal.max()\n",
    "        if canalmax > 0:\n",
    "            factor = 255/canalmax\n",
    "            canal = (canal * factor).astype(int)\n",
    "        img[:,:,i] = canal\n",
    "\n",
    "    # Get and resize test images\n",
    "    X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "    sizes_test = []\n",
    "    print('Getting and resizing test images ... ')\n",
    "    sys.stdout.flush()\n",
    "    for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "        path = TEST_PATH + id_ + '/images/' + id_ + '.png'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #    img = img[:, :, :3]\n",
    "    #     img = imread(path + '/images/' + id_ + '.png', as_grey=True)[:,:,:]\n",
    "        sizes_test.append([img.shape[0], img.shape[1]])\n",
    "        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        X_test[n] = img\n",
    "\n",
    "    for i in range(IMG_CHANNELS):\n",
    "        canal = img[:,:,i]\n",
    "        canal = canal - canal.min()\n",
    "        canalmax = canal.max()\n",
    "        if canalmax > 0:\n",
    "            factor = 255/canalmax\n",
    "            canal = (canal * factor).astype(int)\n",
    "        img[:,:,i] = canal\n",
    "\n",
    "    print('Done!')\n",
    "    \n",
    "    print('Preparing to augment images')\n",
    "    get_more_images(X_train)\n",
    "    dublicate_labels(X_train)\n",
    "    \n",
    "    get_more_images(Y_train)\n",
    "    dublicate_labels(Y_train)\n",
    "    \n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    return X_train, Y_train, X_test\n",
    "\n",
    "def read_train_data_properties(train_dir, img_dir_name, mask_dir_name):\n",
    "    \"\"\"Read basic properties of training images and masks\"\"\"\n",
    "    tmp = []\n",
    "    for i,dir_name in enumerate(next(os.walk(train_dir))[1]):\n",
    "\n",
    "        img_dir = os.path.join(train_dir, dir_name, img_dir_name)\n",
    "        mask_dir = os.path.join(train_dir, dir_name, mask_dir_name) \n",
    "        num_masks = len(next(os.walk(mask_dir))[2])\n",
    "        img_name = next(os.walk(img_dir))[2][0]\n",
    "        img_name_id = os.path.splitext(img_name)[0]\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        mask_path = os.path.join(train_dir,dir_name,mask_dir,img_name_id+'_mask.png')\n",
    "        img_shape = read_image(img_path).shape\n",
    "        tmp.append(['{}'.format(img_name_id), img_shape[0], img_shape[1],\n",
    "                    img_shape[0]/img_shape[1], img_shape[2], num_masks,\n",
    "                    img_path, mask_dir,mask_path])\n",
    "\n",
    "    train_df = pd.DataFrame(tmp, columns = ['img_id', 'img_height', 'img_width',\n",
    "                                            'img_ratio', 'num_channels', \n",
    "                                            'num_masks', 'image_path', 'mask_dir','mask_path'])\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def read_test_data_properties(test_dir, img_dir_name):\n",
    "    \"\"\"Read basic properties of test images.\"\"\"\n",
    "    tmp = []\n",
    "    for i,dir_name in enumerate(next(os.walk(test_dir))[1]):\n",
    "\n",
    "        img_dir = os.path.join(test_dir, dir_name, img_dir_name)\n",
    "        img_name = next(os.walk(img_dir))[2][0]\n",
    "        img_name_id = os.path.splitext(img_name)[0]\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img_shape = read_image(img_path).shape\n",
    "        tmp.append(['{}'.format(img_name_id), img_shape[0], img_shape[1],\n",
    "                    img_shape[0]/img_shape[1], img_shape[2], img_path])\n",
    "\n",
    "    test_df = pd.DataFrame(tmp, columns = ['img_id', 'img_height', 'img_width',\n",
    "                                           'img_ratio', 'num_channels', 'image_path'])\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "def get_domimant_colors(img, top_colors=1):\n",
    "    \"\"\"Return dominant image color\"\"\"\n",
    "    img_l = img.reshape((img.shape[0] * img.shape[1], img.shape[2]))\n",
    "    clt = KMeans(n_clusters = top_colors)\n",
    "    clt.fit(img_l)\n",
    "    # grab the number of different clusters and create a histogram\n",
    "    # based on the number of pixels assigned to each cluster\n",
    "    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n",
    "    (hist, _) = np.histogram(clt.labels_, bins = numLabels)\n",
    "    # normalize the histogram, such that it sums to one\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= hist.sum()\n",
    "    return clt.cluster_centers_, hist\n",
    "\n",
    "def cluster_images_by_hsv():\n",
    "    \"\"\"Clusterization based on hsv colors. Adds 'hsv_cluster' column to tables\"\"\"\n",
    "    print('Loading data')\n",
    "#     x_train_hsv,x_test_hsv = load_raw_data(image_size=None,space='hsv',load_mask=False)\n",
    "    x_train_hsv,x_test_hsv = get_images()\n",
    "    x_hsv = np.concatenate([x_train_hsv,x_test_hsv])\n",
    "    print('Calculating dominant hsv for each image')\n",
    "    dominant_hsv = []\n",
    "    for img in tqdm.tqdm(x_hsv):\n",
    "        res1, res2 = get_domimant_colors(img,top_colors=1)\n",
    "        dominant_hsv.append(res1.squeeze())\n",
    "    print('Calculating clusters')\n",
    "    kmeans = KMeans(n_clusters=3).fit(dominant_hsv)\n",
    "    train_df['HSV_CLUSTER'] = kmeans.predict(dominant_hsv[:len(x_train_hsv)])\n",
    "    test_df['HSV_CLUSTER'] = kmeans.predict(dominant_hsv[len(x_train_hsv):])\n",
    "    print('Images clustered')\n",
    "    return None\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    rotated_images_pos90 = []\n",
    "    rotated_images_neg90 = []\n",
    "    \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "#         IMG_HEIGHT = imgs.shape[0]\n",
    "#         IMG_WIDTH = imgs.shape[1]\n",
    "    \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        arp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        arp90_r = cv2.warpAffine(a,arp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        arn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),270,1)\n",
    "        arn90_r = cv2.warpAffine(a,arn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        brp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        brp90_r = cv2.warpAffine(b,brp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        brn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),270,1)\n",
    "        brn90_r = cv2.warpAffine(b,brn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        crp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        crp90_r = cv2.warpAffine(c,crp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        crn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),270,1)\n",
    "        crn90_r = cv2.warpAffine(c,crn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "        rotated_images_pos90.append(np.dstack((arp90_r, brp90_r, crp90_r)))\n",
    "        rotated_images_neg90.append(np.dstack((arn90_r, brn90_r, crn90_r)))\n",
    "    \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    rp = np.array(rotated_images_pos90)\n",
    "    rn = np.array(rotated_images_neg90)\n",
    "    \n",
    " #   print(v)\n",
    "#     print('h shape \\n' + h)\n",
    "#    print(scp)\n",
    "#     print('rn shape \\n' + rn)\n",
    "    more_images = np.concatenate((imgs,v,h,rp,rn))\n",
    "    \n",
    "    return more_images\n",
    "\n",
    "def duplicate_labels(labels):\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    rotated_images_pos90 = []\n",
    "    rotated_images_neg90 = []\n",
    "    \n",
    "    for i in range(0,labels.shape[0]):\n",
    "        \n",
    "#         IMG_HEIGHT = labels.shape[0]\n",
    "#         IMG_WIDTH = labels.shape[1]\n",
    "        \n",
    "        a=labels[i,:,:,0]\n",
    "\n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        \n",
    "        arp90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),90,1)\n",
    "        arp90_r = cv2.warpAffine(a,arp90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        arn90=cv2.getRotationMatrix2D((IMG_WIDTH/2,IMG_HEIGHT/2),-90,1)\n",
    "        arn90_r = cv2.warpAffine(a,arn90,(IMG_WIDTH,IMG_HEIGHT))\n",
    "        \n",
    "        vert_flip_imgs.append(av.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        hori_flip_imgs.append(ah.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        rotated_images_pos90.append(arp90_r.reshape(IMG_WIDTH,IMG_WIDTH,1))\n",
    "        rotated_images_neg90.append(arn90_r.reshape(IMG_WIDTH,IMG_WIDTH,1))      \n",
    "        \n",
    "        \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    rp = np.array(rotated_images_pos90)\n",
    "    rn = np.array(rotated_images_neg90) \n",
    "    \n",
    "    duplicate_labels = np.concatenate((labels,v,h,rp,rn))\n",
    "    return duplicate_labels\n",
    "\n",
    "get_images()\n",
    "# X_validation = X_train[-50:,...]\n",
    "# Y_validation = Y_train[-50:]\n",
    "# X_train = X_train[0:-50,...]\n",
    "# Y_train = Y_train[0:-50]\n",
    "# print(\"Validation Set of Size \"+str(Y_validation.shape[0])+\" Separated\")\n",
    "# Y_train.dtype = np.uint8\n",
    "# X_train = get_more_images(X_train)\n",
    "# Y_train = duplicate_labels(Y_train)\n",
    "cluster_images_by_hsv()\n",
    "\n",
    "X_train_1cl = train_df[train_df['HSV_CLUSTER'] == 0]\n",
    "X_train_2cl = train_df[train_df['HSV_CLUSTER'] == 1]\n",
    "X_train_3cl = train_df[train_df['HSV_CLUSTER'] == 2]\n",
    "\n",
    "X_test_1cl = test_df[test_df['HSV_CLUSTER'] == 0]\n",
    "X_test_2cl = test_df[test_df['HSV_CLUSTER'] == 1]\n",
    "X_test_3cl = test_df[test_df['HSV_CLUSTER'] == 2]\n",
    "\n",
    "# Y_train_1cl = Y_train\n",
    "# Y_train_2cl = Y_train\n",
    "# Y_train_3cl = Y_train\n",
    "\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)\n",
    "# print (X_validation.shape)\n",
    "# print (Y_validation.shape)\n",
    "# print(\"Data Rotation and Flipping Complete\")\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)\n",
    "# print (X_validation.shape)\n",
    "# print (Y_validation.shape)\n",
    "# X_train = np.concatenate((X_train,X_validation),axis=0)\n",
    "# Y_train = np.concatenate((Y_train,Y_validation),axis=0)\n",
    "#np.savez_compressed(file='train.npz',X=X_train,Y=Y_train)\n",
    "#np.savez_compressed(file='test.npz',X=X_test)\n",
    "\n",
    "# npz = np.load('G:/Kaggle_data/Kaggle_bowl/train.npz') \n",
    "# X_train2 = npz['X'] \n",
    "# Y_train2 = npz['Y']\n",
    "# X_test2 = np.load('G:/Kaggle_data/Kaggle_bowl/test.npz')['X']\n",
    "\n",
    "# Check if training data looks all right\n",
    "# ix = random.randint(0, len(train_ids))\n",
    "# imshow(X_train[ix])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[ix]))\n",
    "# plt.show()\n",
    "\n",
    "# Define IoU metric\n",
    "\"\"\"def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\"\"\"\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "# # Build U-Net model\n",
    "# inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "# s = Lambda(lambda x: x / 255) (inputs)\n",
    "\n",
    "# c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "# c1 = Dropout(0.1) (c1)\n",
    "# c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "# p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "# c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "# c2 = Dropout(0.1) (c2)\n",
    "# c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "# p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "# c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "# c3 = Dropout(0.2) (c3)\n",
    "# c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "# p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "# c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "# c4 = Dropout(0.2) (c4)\n",
    "# c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "\n",
    "# u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c4)\n",
    "# u6 = concatenate([u6, c3])\n",
    "# c6 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "# c6 = Dropout(0.2) (c6)\n",
    "# c6 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "# u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "# u7 = concatenate([u7, c2])\n",
    "# c7 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "# c7 = Dropout(0.2) (c7)\n",
    "# c7 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "# u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "# u8 = concatenate([u8, c1])\n",
    "# c8 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "# c8 = Dropout(0.1) (c8)\n",
    "# c8 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "\n",
    "# outputs = Conv2D(1, (1, 1), activation='sigmoid') (c8)\n",
    "\n",
    "# model = Model(inputs=[inputs], outputs=[outputs])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])\n",
    "# model.summary()\n",
    "\n",
    "# # # Fit model\n",
    "# # earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "# # checkpointer = ModelCheckpoint('model-dsbowl2018-1_+img_rot_corr_256_UNET-sh.h5', verbose=1, save_best_only=True)\n",
    "# # results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=16, epochs=50, \n",
    "# #                     callbacks=[earlystopper, checkpointer])\n",
    "# # # results2 = model.fit(X_train2, Y_train2, validation_split=0.1, batch_size=16, epochs=50, \n",
    "# # #                     callbacks=[earlystopper, checkpointer])\n",
    "\n",
    "# Predict on train, val and test\n",
    "model_0cl = load_model('model-dsbowl2018-1_+img_rot_corrected_0cl.h5', custom_objects={'dice_coef': dice_coef})\n",
    "model_1cl = load_model('model-dsbowl2018-1_+img_rot_corrected_1cl.h5', custom_objects={'dice_coef': dice_coef})\n",
    "model_2cl = load_model('model-dsbowl2018-1_+img_rot_corrected_2cl.h5', custom_objects={'dice_coef': dice_coef})\n",
    "\n",
    "preds_train_0cl = model_0cl.predict(X_train_0cl[:int(X_train_0cl.shape[0]*0.9)], verbose=1)\n",
    "preds_val_0cl = model_0cl.predict(X_train_0cl[int(X_train_0cl.shape[0]*0.9):], verbose=1)\n",
    "preds_test_0cl = model_0cl.predict(X_test_0cl, verbose=1)\n",
    "\n",
    "preds_train_1cl = model_1cl.predict(X_train_1cl[:int(X_train_1cl.shape[0]*0.9)], verbose=1)\n",
    "preds_val_1cl = model_1cl.predict(X_train_1cl[int(X_train_1cl.shape[0]*0.9):], verbose=1)\n",
    "preds_test_1cl = model_1cl.predict(X_test_1cl, verbose=1)\n",
    "\n",
    "preds_train_2cl = model_2cl.predict(X_train_2cl[:int(X_train_2cl.shape[0]*0.9)], verbose=1)\n",
    "preds_val_2cl = model_2cl.predict(X_train_2cl[int(X_train_2cl.shape[0]*0.9):], verbose=1)\n",
    "preds_test_2cl = model_2cl.predict(X_test_2cl, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# Threshold predictions\n",
    "preds_train_t0 = (preds_train_0cl > 0.5).astype(np.uint8)\n",
    "preds_val_t0 = (preds_val_0cl > 0.5).astype(np.uint8)\n",
    "preds_test_t0 = (preds_test_0cl > 0.5).astype(np.uint8)\n",
    "\n",
    "preds_train_t1 = (preds_train_1cl > 0.5).astype(np.uint8)\n",
    "preds_val_t1 = (preds_val_1cl > 0.5).astype(np.uint8)\n",
    "preds_test_t1 = (preds_test_1cl > 0.5).astype(np.uint8)\n",
    "\n",
    "preds_train_t2 = (preds_train_2cl > 0.5).astype(np.uint8)\n",
    "preds_val_t2 = (preds_val_2cl > 0.5).astype(np.uint8)\n",
    "preds_test_t2 = (preds_test_2cl > 0.5).astype(np.uint8)\n",
    "\n",
    "# Create list of upsampled test masks\n",
    "preds_test_upsampled = []\n",
    "for i in range(len(preds_test_t0)):\n",
    "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
    "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
    "                                       mode='constant', preserve_range=True))\n",
    "    \n",
    "for i in range(len(preds_test_t1)):\n",
    "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
    "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
    "                                       mode='constant', preserve_range=True))\n",
    "    \n",
    "for i in range(len(preds_test_t2)):\n",
    "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
    "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
    "                                       mode='constant', preserve_range=True))\n",
    "\n",
    "    \n",
    "# Perform a sanity check on some random training samples\n",
    "# ix = random.randint(0, len(preds_train_t))\n",
    "# imshow(X_train[ix])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[ix]))\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(preds_train_t[ix]))\n",
    "# plt.show()\n",
    "\n",
    "# Perform a sanity check on some random validation samples\n",
    "# ix = random.randint(0, len(preds_val_t))\n",
    "# imshow(X_train[int(X_train.shape[0]*0.9):][ix])\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(Y_train[int(Y_train.shape[0]*0.9):][ix]))\n",
    "# plt.show()\n",
    "# imshow(np.squeeze(preds_val_t[ix]))\n",
    "# plt.show()\n",
    "\n",
    "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def prob_to_rles(x, cutoff=0.5):\n",
    "    lab_img = label(x > cutoff)\n",
    "    for i in range(1, lab_img.max() + 1):\n",
    "        yield rle_encoding(lab_img == i)\n",
    "        \n",
    "new_test_ids = []\n",
    "rles = []\n",
    "for n, id_ in enumerate(test_ids):\n",
    "    rle = list(prob_to_rles(preds_test_upsampled[n]))\n",
    "    rles.extend(rle)\n",
    "    new_test_ids.extend([id_] * len(rle))\n",
    "    \n",
    "# Create submission DataFrame\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ImageId'] = new_test_ids\n",
    "sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
    "sub.to_csv('sub-dsbowl2018-1_%s_256_+flip_rot_corr_256_final.csv' % time, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
